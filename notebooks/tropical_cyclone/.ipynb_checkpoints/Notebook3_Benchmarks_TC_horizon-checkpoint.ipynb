{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2f5deb-46a8-4d65-a499-c87040b318ec",
   "metadata": {},
   "source": [
    "# Optimize distance : forecasting of tropical cylone data as a function of horizon\n",
    "\n",
    "This notebook accompanies the following publication:\n",
    "Paul Platzer, Arthur Avenas, Bertrand Chapron, Lucas Drumetz, Alexis Mouche, Léo Vinour. Distance Learning for Analog Methods. 2024. [⟨hal-04841334⟩](https://hal.science/hal-04841334)\n",
    "\n",
    "It is used to plot the result of optimization algorithms for numerical experiments with IBTrACS tropical cyclone data, varying the forecast horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c62aac-23c0-471c-b2f8-66dd6947fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"16\"\n",
    "import sys\n",
    "sys.path.append('../../functions/.')\n",
    "from analogs import apply_transform, find_analogues, compute_weights, compute_diffs, compute_mae_mad, compute_error\n",
    "from TC_utils import M, Rmax_from_M, correct_vmx_ibt, Rmxa23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0bfcda-a0f8-4ae9-b1ad-7ccd3fe45c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "cols = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c478b0-c69c-419e-962b-3978f39c7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../data/tropical_cyclone/'\n",
    "output_folder = '../../output/tropical_cyclone/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db3468-0d49-4844-812d-ea787fe2edeb",
   "metadata": {},
   "source": [
    "# Parameters for loading IBTrACS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a3ab52-cc20-4b66-baea-168eee4bc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(data_folder)\n",
    "\n",
    "# Input variables\n",
    "var_names = ['Vmax', 'Rmax_IBT', 'R34', 'fcor', 'u_trans', 'v_trans']\n",
    "\n",
    "# Output variable to forecast: derivative of Vmax\n",
    "var_y = ['Vmax']\n",
    "dydt = True\n",
    "ind_var_y = []\n",
    "for name_tmp in var_y:\n",
    "    ind_var_y.append(np.argwhere(np.array(var_names)==name_tmp)[0][0])\n",
    "\n",
    "# Utils to compute Rmax estimate from Avenas et al. (2023)\n",
    "var_A23 = ['fcor', 'Vmax', 'R34', ]\n",
    "ind_A23 = []\n",
    "for name_tmp in var_A23:\n",
    "    ind_A23.append(np.argwhere(np.array(var_names)==name_tmp)[0][0])\n",
    "\n",
    "# Add names of auxilliary variables (Rmax_A23 and time-derivatives)\n",
    "var_names_all = var_names.copy()\n",
    "var_names_all.append('Rmax_A23')\n",
    "for name in var_names_all.copy():\n",
    "    var_names_all.append('d'+name+'/dt')\n",
    "\n",
    "# Add name of time since the threshold of 18m/s is crossed for Vmax\n",
    "var_names_all.append('t_18')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613d011-31e5-4c06-becf-883d01119938",
   "metadata": {},
   "source": [
    "# Load optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c77c57f-230e-47cc-addd-4bf450fb4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of gradient-descent optimization\n",
    "npzfile = np.load(output_folder + 'optim_TC_horizon.npz')\n",
    "var_y = npzfile['var_y']\n",
    "var_names_all = npzfile['var_names_all']\n",
    "k = npzfile['k']\n",
    "corr_length_train = npzfile['corr_length_train']\n",
    "A_grad = npzfile['A']\n",
    "# E_grad_train = npzfile['E_train']\n",
    "E_grad_batch = npzfile['E_batch']\n",
    "E_grad_test = npzfile['E_test']\n",
    "E_unoptim_test = E_grad_test[:,:,0]\n",
    "hh = npzfile['hh']\n",
    "random_state_number = npzfile['random_state_number']\n",
    "Nperm = len(random_state_number)\n",
    "nn_algo = 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60baeba3-30c8-495d-8ea4-2accb54703b5",
   "metadata": {},
   "source": [
    "# Also compute simple benchmarks\n",
    "Climatology and persistence + compute errors on training set if not already done during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fda1b9-60ee-4e47-a59b-5c11dbaea071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85306dde16794d39958c7db5c22c3e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29349515a47e4a59aa62b85089652048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a7caf79f324ea0a0f3c8316d6c4b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f970321e594660a0479c6f950f398a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f32891ae380459d8ecfbedc019dce86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107be83ce98d4b7bbc5481e2301908e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1854b0d9771467996618207db5850d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58000d1c793461eb5433e40ac29956e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9067e87add4038a328bdcd2f60e8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e3f71e27f844108c5d47e6306aee7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700f3dfb4a144351906c9d639a3a957a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E_unoptim_train = []\n",
    "E_grad_train = []\n",
    "scale_dV = []\n",
    "MAE_pers = []\n",
    "CRPS_clim = []\n",
    "\n",
    "# Loop on forecast horizon\n",
    "for l in tqdm(range(len(hh))):\n",
    "\n",
    "    h = hh[l]\n",
    "\n",
    "    # Load IBTrACS dataset\n",
    "    IBT = np.array(pandas.read_csv(data_folder + files[0], usecols = var_names))\n",
    "    IBT = np.concatenate( [ IBT , \n",
    "             Rmxa23(IBT[:,ind_A23[0]] , IBT[:,ind_A23[1]] , IBT[:,ind_A23[2]]).reshape(-1,1) ,\n",
    "                          ],  axis=1)\n",
    "    IBT = np.concatenate( ( IBT[1:] , IBT[1:] - IBT[:-1] ) , axis=1 )\n",
    "    IBT = np.concatenate( [ IBT ,\n",
    "               3*np.arange(len(IBT)).reshape(-1,1) ],  axis=1)\n",
    "    train_x = IBT[0:-h,:]\n",
    "    train_y = IBT[h:,ind_var_y] - IBT[0:-h,ind_var_y] \n",
    "    ID = np.array([0]*len(IBT[0:-h,:]))\n",
    "    \n",
    "\n",
    "    for i in np.arange(1, len(files)):\n",
    "        IBT = np.array(pandas.read_csv(data_folder + files[i], usecols = var_names))\n",
    "        IBT = np.concatenate( [ IBT , \n",
    "                 Rmxa23(IBT[:,ind_A23[0]] , IBT[:,ind_A23[1]] , IBT[:,ind_A23[2]]).reshape(-1,1) ,\n",
    "                              ],  axis=1)\n",
    "        IBT = np.concatenate( ( IBT[1:] , IBT[1:] - IBT[:-1] ) , axis=1 )\n",
    "        IBT = np.concatenate( [ IBT ,\n",
    "               3*np.arange(len(IBT)).reshape(-1,1) ],  axis=1)\n",
    "        train_x = np.concatenate([train_x, IBT[0:-h,:]])\n",
    "        train_y = np.concatenate([train_y, IBT[h:,ind_var_y] - IBT[0:-h,ind_var_y]])\n",
    "        ID = np.concatenate([ID, np.array([i]*len(IBT[0:-h,:]))])\n",
    "\n",
    "    # Center and reduce\n",
    "    mean_IBTrACS = np.mean(train_x, axis=0)\n",
    "    std_IBTrACS = np.std(train_x, axis=0)\n",
    "    mean_y = np.mean(train_y, axis=0)\n",
    "    std_y = np.std(train_y, axis=0)\n",
    "    for j in range(train_x.shape[1]):\n",
    "        train_x[:,j] = (train_x[:,j] - mean_IBTrACS[j]) / std_IBTrACS[j]\n",
    "    for j in range(train_y.shape[1]):\n",
    "        train_y[:,j] = (train_y[:,j] - mean_y[j]) / std_y[j]\n",
    "        \n",
    "    scale_dV.append( std_y )\n",
    "    \n",
    "    E_unoptim_train_perm = []\n",
    "    E_grad_train_perm = []\n",
    "    MAE_pers_perm = []\n",
    "    CRPS_clim_perm = []\n",
    "    for j in tqdm(range(Nperm)):\n",
    "        # Generate random permutation (reproducible)\n",
    "        rs = np.random.RandomState(random_state_number[j])\n",
    "        perm = rs.permutation(len(files))\n",
    "        Itest = np.argwhere(np.isin(ID, perm[:len(files)//3]))[:,0]\n",
    "        Itrain = np.argwhere(np.isin(ID, perm[len(files)//3:]))[:,0]\n",
    "\n",
    "        # Compute train-set CRPS of unoptimized distance\n",
    "        nn = NearestNeighbors( algorithm = nn_algo , \n",
    "                                  n_neighbors = k + 1 + 2*corr_length_train ) # leave-one-out procedure + anticipating time-correlated data\n",
    "        nn.fit(train_x[Itrain])\n",
    "        E_unoptim_train_perm.append( compute_error(train_x[Itrain], train_y, Itrain, Itrain, k, nn,\n",
    "                                                loo=True, corr_length_train=corr_length_train, error_type='CRPS') )\n",
    "\n",
    "        # Compute train-set CRPS of gradient-descent optimized distance (if not computed during the optimization process)\n",
    "        train_X = apply_transform(train_x, A_grad[l, j, -1], Itrain)\n",
    "        nn = NearestNeighbors( algorithm = nn_algo , \n",
    "                                  n_neighbors = k + 1 + 2*corr_length_train ) # leave-one-out procedure + anticipating time-correlated data\n",
    "        nn.fit(train_X)        \n",
    "        E_grad_train_perm.append( compute_error(train_X, train_y, Itrain, Itrain, k, nn,\n",
    "                                                loo=True, corr_length_train=corr_length_train, error_type='CRPS') )\n",
    "        \n",
    "        # Compare with persistence forecast (for which the CRPS equals the Mean Absolute Error MAE of a \"zero\" forecast)\n",
    "        test_y = train_y[Itest]\n",
    "        MAE_pers_perm.append( np.mean(np.abs(test_y)) )\n",
    "    \n",
    "        # Compare with CRPS of climatological prediction\n",
    "        subsamp2 = 10 # subsampling parameter to make computation faster\n",
    "        crps_clim_pred = .5*np.mean(\n",
    "            np.abs( np.repeat(test_y[::subsamp2, np.newaxis], len(test_y[::subsamp2]), axis=1)\n",
    "               - np.transpose( np.repeat(test_y[::subsamp2, np.newaxis], len(test_y[::subsamp2]), axis=1) ) ) \n",
    "        )\n",
    "        CRPS_clim_perm.append(crps_clim_pred.copy())\n",
    "\n",
    "    E_unoptim_train.append(E_unoptim_train_perm)\n",
    "    E_grad_train.append(E_grad_train_perm)\n",
    "    MAE_pers.append(MAE_pers_perm)\n",
    "    CRPS_clim.append(CRPS_clim_perm)\n",
    "\n",
    "E_unoptim_train = np.array(E_unoptim_train)\n",
    "E_grad_train = np.array(E_grad_train)\n",
    "MAE_pers = np.array(MAE_pers)\n",
    "CRPS_clim = np.array(CRPS_clim)\n",
    "scale_dV = np.array(scale_dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594e469-2540-4a84-bbff-d0555d85fd26",
   "metadata": {},
   "source": [
    "Save (computations can be long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e9d0a2-e22f-41c1-ae22-a4ce0965fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.savez(output_folder + 'benchmarks_for_plot_horizon.npz',\n",
    "        E_unoptim_train = E_unoptim_train,\n",
    "        E_grad_train = E_grad_train,\n",
    "        MAE_pers = MAE_pers,\n",
    "        CRPS_clim = CRPS_clim,\n",
    "        scale_dV = scale_dV,\n",
    "         random_state_number = random_state_number,\n",
    "         hh = hh,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d7e73-5342-4f4f-b0da-8249d02a9a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
